{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9417 19T3  Homework 2: Applying and Implementing Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 â€“ Overfitting avoidance \n",
    "\n",
    "Dealing with noisy data is a key issue in machine learning. Unfortunately, even algorithms that have noise-handling mechanisms built-in, like decision trees, can overfit noisy data, unless their \"overfitting avoidance\" or *regularization* hyper-parameters are set properly.\n",
    "\n",
    "You will be using datasets that have had various amounts of \"class noise\" added\n",
    "by randomly changing the actual class value to a different one for a\n",
    "specified percentage of the training data.\n",
    "Here we will specify three arbitrarily chosen levels of noise: low\n",
    "($25\\%$), medium ($50\\%$) and high ($75\\%$).\n",
    "The learning algorithm must try to \"see through\" this noise and learn\n",
    "the best model it can, which is then evaluated on test data *without*\n",
    "added noise to evaluate how well it has avoided fitting the noise.\n",
    "\n",
    "We will also let the algorithm do a limited _grid search_ using cross-validation\n",
    "for the best *over-fitting avoidance* parameter settings on each training set.\n",
    "\n",
    "### Running the classifiers\n",
    "\n",
    "**1(a). [0.5 mark]** \n",
    "\n",
    "Run the code section in the notebook cells below. This will generate a table of results, which you should copy and paste **WITHOUT MODIFICATION** into you report as your answer for \"Question 1(a)\". \n",
    "\n",
    "The output of the code section is a table, which represents the percentage accuracy of classification for the decision tree algorithm. The first column contains the result of the \"Default\" classifier, which is the decision tree algorithm with default parameter settings running on each of the datasets which have had $50\\%$ noise added. From the second column on, in each column the results are obtained by running the decision tree algorithm on $0\\%$, $25\\%$, $50\\%$ and $75\\%$ noise added to each of the datasets, and in the parentheses is shown the result of a [grid search](http://en.wikipedia.org/wiki/Hyperparameter_optimization) that has been applied to determine the best value for a basic parameter of the decision tree algorithm, namely [max_depth](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) i.e., The maximum depth of the tree. \n",
    "\n",
    "### Result interpretation\n",
    "Answer these questions in your report file.  Your answers must be based on the results table you saved in \"Question 1(a)\".\n",
    "\n",
    "**1(b). [0.5 mark]** Refer to Homework2.pdf file.\n",
    "\n",
    "**1(c). [0.5 mark]** Refer to Homework2.pdf file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for question 1\n",
    "\n",
    "It is only necessary to run the following code to answer the question, but you should also go through it to make sure you know what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for question 1\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def label_enc(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    return le\n",
    "\n",
    "def features_encoders(features,categorical_features='all'):\n",
    "    n_samples, n_features = features.shape\n",
    "    label_encoders = [preprocessing.LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "    X_int = np.zeros_like(features, dtype=np.int)\n",
    "\n",
    "    for i in range(n_features):\n",
    "        feature_i = features[:, i]\n",
    "        label_encoders[i].fit(feature_i)\n",
    "        X_int[:, i] = label_encoders[i].transform(feature_i)\n",
    "        \n",
    "    enc = preprocessing.OneHotEncoder(categorical_features=categorical_features)\n",
    "    return enc.fit(X_int),label_encoders\n",
    "\n",
    "def feature_transform(features,label_encoders, one_hot_encoder):\n",
    "    \n",
    "    n_samples, n_features = features.shape\n",
    "    X_int = np.zeros_like(features, dtype=np.int)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        feature_i = features[:, i]\n",
    "        X_int[:, i] = label_encoders[i].transform(feature_i)\n",
    "\n",
    "    return one_hot_encoder.transform(X_int).toarray()\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    dataset = arff.loadarff(path)\n",
    "    data = pd.DataFrame(dataset[0])\n",
    "    attr = np.array(data.columns)\n",
    "    data = DataFrameImputer().fit_transform(data).values\n",
    "\n",
    "    # mask categorical features\n",
    "    masks = []\n",
    "    for i in range(len(attr)-1):\n",
    "        if attr[i][1] != 'REAL':\n",
    "            masks.append(i)\n",
    "    return data, masks\n",
    "\n",
    "def preprocess(data,masks, noise_ratio):\n",
    "    # split data\n",
    "    train_data, test_data = train_test_split(data,test_size=0.3,random_state=0)\n",
    "\n",
    "    # test data\n",
    "    test_features = test_data[:,0:test_data.shape[1]-1]\n",
    "    test_labels = test_data[:,test_data.shape[1]-1]\n",
    "\n",
    "    # training data\n",
    "    features = train_data[:,0:train_data.shape[1]-1]\n",
    "    labels = train_data[:,train_data.shape[1]-1]\n",
    "\n",
    "    classes = list(set(labels))\n",
    "    # categorical features need to be encoded\n",
    "    if len(masks):\n",
    "        one_hot_enc, label_encs = features_encoders(data[:,0:data.shape[1]-1],masks)\n",
    "        test_features = feature_transform(test_features,label_encs,one_hot_enc)\n",
    "        features = feature_transform(features,label_encs,one_hot_enc)\n",
    "\n",
    "    le = label_enc(data[:,data.shape[1]-1])\n",
    "    labels = le.transform(train_data[:,train_data.shape[1]-1])\n",
    "    test_labels = le.transform(test_data[:,test_data.shape[1]-1])\n",
    "    \n",
    "    # add noise\n",
    "    np.random.seed(1234)\n",
    "    noise = np.random.randint(len(classes)-1, size=int(len(labels)*noise_ratio))+1\n",
    "    \n",
    "    noise = np.concatenate((noise,np.zeros(len(labels) - len(noise),dtype=np.int)))\n",
    "    labels = (labels + noise) % len(classes)\n",
    "\n",
    "    return features,labels,test_features,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "paths = ['australian','labor','diabetes','ionosphere']\n",
    "noise = [0,0.25,0.5,0.75]\n",
    "\n",
    "scores = []\n",
    "params = []\n",
    "\n",
    "for path in paths:\n",
    "    score = []\n",
    "    param = []\n",
    "    path += '.arff'\n",
    "    data, masks = load_data(path)\n",
    "    \n",
    "    # training on data with 50% noise and default parameters\n",
    "    features, labels, test_features, test_labels = preprocess(data, masks, 0.5)\n",
    "    tree = DecisionTreeClassifier(random_state=0,max_depth=2, min_impurity_decrease=0)    \n",
    "    tree.fit(features, labels)\n",
    "    tree_preds = tree.predict(test_features)\n",
    "    tree_performance = accuracy_score(test_labels, tree_preds)\n",
    "    score.append(tree_performance)\n",
    "    param.append(tree.get_params()['max_depth'])\n",
    "    \n",
    "    # training on data with noise levels of 0%, 25%, 50% and 75%\n",
    "    for noise_ratio in noise:\n",
    "        features, labels, test_features, test_labels = preprocess(data, masks, noise_ratio)\n",
    "        param_grid = {'max_depth': np.arange(2,30,5)}\n",
    "\n",
    "        grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid,cv=10,return_train_score=True)\n",
    "        grid_tree.fit(features, labels)\n",
    "\n",
    "        estimator = grid_tree.best_estimator_\n",
    "        tree_preds = grid_tree.predict(test_features)\n",
    "        tree_performance = accuracy_score(test_labels, tree_preds)\n",
    "        score.append(tree_performance)\n",
    "        param.append(estimator.get_params()['max_depth'])\n",
    "\n",
    "    scores.append(score)\n",
    "    params.append(param)\n",
    "\n",
    "# print the results\n",
    "header = \"{:^112}\".format(\"Decision Tree Results\") + '\\n' + '-' * 112  + '\\n' + \\\n",
    "\"{:^15} | {:^16} | {:^16} | {:^16} | {:^16} | {:^16} |\".format(\"Dataset\", \"Default\", \"0%\", \"25%\", \"50%\", \"75%\") + \\\n",
    " '\\n' + '-' * 112  + '\\n'\n",
    "\n",
    "# print result table\n",
    "print(header)\n",
    "for i in range(len(scores)):\n",
    "    #scores = score_list[i][1]\n",
    "    print(\"{:<16}\".format(paths[i]),end=\"\")\n",
    "    for j in range(len(params[i])):\n",
    "        print(\"|  {:>6.2%} ({:>2})     \" .format(scores[i][j],params[i][j]),end=\"\")\n",
    "    print('|\\n')\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
